1. 線性與代數、空間與向量空間
線性代數中的『線性』(Linear) 指的是什麼？
『線性』在數學上描述的是一種平直且均勻的性質。嚴格定義包含兩個條件：
加法性 (Additivity)： $f(x + y) = f(x) + f(y)$
齊次性 (Homogeneity)： $f(cx) = c \cdot f(x)$
直觀理解：
如果您畫圖，圖形必須是通過原點的直線（1D）、平面（2D）或超平面。
輸入變大 2 倍，輸出也變大 2 倍；輸入兩個東西的總和，等於個別輸入後再相加。也就是說，系統沒有「彎曲」（像 $x^2$）也沒有「常數偏移」（像 $+b$）。
為何要稱為『代數』(Algebra)？
代數不僅僅是算術（具體的數字運算）。代數是用符號代表數，並研究這些符號在特定運算結構下的規則。線性代數就是研究「線性結構」下的運算規則（如矩陣乘法、向量加法）。
數學中的『空間』(Space) 是什麼？
在數學中，『空間』指的是一個集合 (Set) 加上定義在該集合上的一組規則 (Structure)。例如，沒有規則的一堆點只是集合，但如果定義了距離，就變成度量空間；如果定義了鄰域，就變成拓撲空間。
為何『向量空間』(Vector Space) 被稱為空間？
因為它定義了一個集合（向量），並賦予了兩條核心規則：向量加法和純量乘法。
在這個空間裡，無論你怎麼將向量相加或縮放，結果仍然在這個空間內（封閉性）。它就像是一個「遊樂場」，只要你遵守線性規則，你永遠不會掉出這個場地。
2. 矩陣與幾何變換
矩陣和向量之間有何關係？矩陣代表的意義是什麼？
向量 (Vector)： 是數據或狀態的載體（幾何上是一個點或箭頭）。
矩陣 (Matrix)： 是函數或運算子。
矩陣代表的意義：
矩陣代表一種線性變換 (Linear Transformation)。當矩陣 $A$ 乘以向量 $x$ ($Ax$) 時，意思是矩陣 $A$ 抓住了向量 $x$，並將其扭曲、旋轉或拉伸到新的位置。矩陣的每一列（column）其實描述了基底向量 (Basis vectors) 變換後的去向。
如何用矩陣代表 2D / 3D 的幾何操作？
在線性代數中，標準的 $2\times2$ (2D) 或 $3\times3$ (3D) 矩陣只能處理固定原點的操作（線性變換）。
縮放 (Scaling)： 對角線上的值控制軸向的拉伸。

$$\begin{bmatrix} s_x & 0 \\ 0 & s_y \end{bmatrix}$$
旋轉 (Rotation)： 使用三角函數（以 2D 逆時針旋轉 $\theta$ 為例）。

$$\begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}$$
平移 (Translation) - 特殊情況：
純粹的線性變換無法做平移，因為平移會移動原點（違反線性定義）。為了解決這個問題，我們引入齊次座標 (Homogeneous Coordinates)，將維度 $+1$。
2D 平移 $(t_x, t_y)$： 使用 $3\times3$ 矩陣。

$$\begin{bmatrix} 1 & 0 & t_x \\ 0 & 1 & t_y \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \\ 1 \end{bmatrix} = \begin{bmatrix} x + t_x \\ y + t_y \\ 1 \end{bmatrix}$$
3. 行列式 (Determinant)
行列式的意義是什麼？
行列式 $\det(A)$ 是一個數值，代表線性變換對空間體積（或面積）的縮放比例。
$\det(A) = 2$：變換後，圖形面積變大 2 倍。
$\det(A) = 0$：空間被壓扁了（例如 2D 壓成一條線），這意味著矩陣不可逆。
$\det(A) < 0$：空間被翻轉了（像鏡像一樣）。
行列式與體積的關係
在 3D 中，$3\times3$ 矩陣形成的平行六面體體積，絕對等於 $|\det(A)|$。
計算方法：
遞迴公式 (Laplace Expansion)：
沿著某一行或列展開。例如 $3\times3$ 矩陣，會拆解成 3 個 $2\times2$ 行列式的加減和。這在計算機上效率極低 ($O(n!)$)。
透過對角化 (Diagonalization)：
如果矩陣能被對角化為 $A = PDP^{-1}$，其中 $D$ 是對角矩陣（對角線上是特徵值 $\lambda$）。

$$\det(A) = \det(P)\det(D)\det(P^{-1}) = \det(D) = \lambda_1 \cdot \lambda_2 \cdots \lambda_n$$

行列式等於所有特徵值的乘積。
用 LU 分解快速計算：
這是電腦最常用的方法。將矩陣分解為下三角 ($L$) 和上三角 ($U$)：$A = LU$。

$$\det(A) = \det(L) \cdot \det(U)$$

通常 $L$ 的對角線為 1（$\det(L)=1$），$U$ 的對角線是主元 (pivots)。
因此，$\det(A) = U \text{ 對角線元素的乘積}$。這將複雜度降到 $O(n^3)$。
4. 特徵值、特徵向量與分解
特徵值 (Eigenvalue) 和特徵向量 (Eigenvector) 的意義
對於矩陣 $A$，如果存在非零向量 $v$ 和純量 $\lambda$ 使得：


$$Av = \lambda v$$
特徵向量 ($v$)： 在變換過程中，方向保持不變的向量（只會變長或變短，不會旋轉）。它們是矩陣變換的「主軸」。
特徵值 ($\lambda$)： 該向量被拉伸或壓縮的倍數。
特徵值分解 (Eigendecomposition) 的用途
將矩陣分解為 $A = PDP^{-1}$。
用途： 極大簡化矩陣的次方運算（$A^k = P D^k P^{-1}$）、解微分方程系統、分析系統穩定性（看 $\lambda$ 是否大於 1）。
5. QR 分解與特徵值算法
QR 分解是什麼？
任何實數矩陣 $A$ 都可以分解為：


$$A = Q R$$
$Q$ (Orthogonal Matrix)： 正交矩陣（$Q^T Q = I$），代表旋轉或反射。其列向量彼此垂直且長度為 1。
$R$ (Upper Triangular Matrix)： 上三角矩陣。
這本質上就是著名的 Gram-Schmidt 正交化過程的矩陣形式。
如何反覆用 QR 分解，完成特徵值分解？ (QR Algorithm)
這是計算特徵值最著名的算法：
令 $A_0 = A$
對 $A_k$ 做 QR 分解：$A_k = Q_k R_k$
將其反向相乘得到下一個矩陣：$A_{k+1} = R_k Q_k$
重複步驟 2-3。
原理： $A_{k+1}$ 與 $A_k$ 是相似矩陣（擁有相同的特徵值）。隨著迭代，$A_k$ 會收斂成一個上三角矩陣（Schur form），其對角線上的元素就是特徵值。
6. SVD 與 PCA
SVD 分解 (Singular Value Decomposition) 是什麼？
特徵值分解只適用於方陣，且不一定存在正交基底。SVD 是線性代數的「瑞士刀」，適用於任何形狀的矩陣 $A$ ($m \times n$)：


$$A = U \Sigma V^T$$
$U$：$m \times m$ 正交矩陣（左奇異向量）。
$\Sigma$：$m \times n$ 對角矩陣（奇異值，非負實數，由大到小排列）。
$V^T$：$n \times n$ 正交矩陣（右奇異向量）。
與特徵值的關係：
$\Sigma$ 中的奇異值是 $A^T A$ 的特徵值的平方根。
$U$ 是 $A A^T$ 的特徵向量；$V$ 是 $A^T A$ 的特徵向量。
主成分分析 (PCA) 是什麼？與 SVD 的關係？
PCA 是一種統計與機器學習技術，用於降維。它試圖找到數據變異量（Variance）最大的方向，稱為主成分。
關係：
PCA 的計算核心其實就是 SVD。
將數據矩陣 $X$ 中心化（減去平均值）。
對 $X$ 做 SVD 分解：$X = U \Sigma V^T$。
$V$ 的行向量 (Columns of V) 就是主成分方向（Principal Components）。
$\Sigma$ (奇異值) 的大小代表了數據在該方向上的變異量大小（重要性）。
透過保留前 $k$ 大的奇異值對應的成分，我們就能在盡量保留資訊的前提下，將高維數據壓縮到低維空間。

